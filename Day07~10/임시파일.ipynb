{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "임시파일.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX0cxWY3ScYh"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "random.seed(1)\n",
        "np.random.seed(1)\n",
        "tf.random.set_seed(1)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paSwW7JhUTxk",
        "outputId": "96e0cbb3-b979-44cf-e3b7-674c2965c24f"
      },
      "source": [
        "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "#conver data type to float32\n",
        "x_train, x_test = tf.cast(x_train, tf.float32), tf.cast(x_test, tf.float32)\n",
        "# x_train = x_train.astype('float32')\n",
        "y_train, y_test = tf.cast(y_train,tf.int64 ),tf.cast(y_test, tf.int64)\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "# for i in range(10):\n",
        "#   plt.imshow(x_train[i], cmap='gray')\n",
        "#   print(y_train[i])\n",
        "#   plt.show()\n",
        "num_classes = 10\n",
        "#num_features = 28*28 =784\n",
        "num_features = x_train.shape[1]*x_train.shape[2]\n",
        "print(num_features)\n",
        "\n",
        "# Flatten images to 1-D vector of 784 features (28*28)\n",
        "x_train , x_test = tf.reshape(x_train, [-1, num_features]) , tf.reshape(x_test, [-1, num_features])\n",
        "# Normalize images value from [0, 255] to [0,1]\n",
        "x_train , x_test = x_train/255 , x_test/255"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se0VQdKFUzY-"
      },
      "source": [
        "from tensorflow.keras import Model, layers \n",
        "\n",
        "class NeuralNet(Model):\n",
        "  def __init__(self):\n",
        "    super(NeuralNet, self).__init__()\n",
        "    self.fc1 = layers.Dense(128, activation=tf.nn.relu)\n",
        "    self.fc2 = layers.Dense(256, activation=tf.nn.relu)\n",
        "    self.out = layers.Dense(num_classes)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.out(x)\n",
        "    return x \n",
        "\n",
        "mlp = NeuralNet()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glvgjPR9fEhe"
      },
      "source": [
        "def cross_entropy_loss(x,y):\n",
        "  loss =tf.nn.sparse_softmax_cross_entropy_with_logits(logits=x , labels=y)\n",
        "  return tf.reduce_mean(loss)\n",
        "\n",
        "def accuracy(y_pred, y_true):\n",
        "  correct_prediction = tf.equal(tf.argmax(y_pred,1), tf.cast(y_true, tf.int64))\n",
        "  return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UEuF-oshyUO"
      },
      "source": [
        "lr = 0.003\n",
        "optimizer = tf.optimizers.SGD(lr)\n",
        "\n",
        "def run_optimization(x,y):\n",
        "  with tf.GradientTape() as g:\n",
        "    pred = mlp(x)\n",
        "    loss = cross_entropy_loss(pred,y)\n",
        "\n",
        "  trainable_variables = mlp.trainable_variables\n",
        "  gradients = g.gradient(loss, trainable_variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYm5TtYzif7W",
        "outputId": "c9c7affb-ca52-44d8-c3f6-ceba7fed1762"
      },
      "source": [
        "batch_size = 200\n",
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_data = train_data.shuffle(60000).batch(batch_size).prefetch(1)\n",
        "\n",
        "epoch = 50\n",
        "display_epoch = 2\n",
        "\n",
        "for epo in range(1, epoch+1):\n",
        "  for step, (batch_x, batch_y) in enumerate(train_data, 1):\n",
        "    run_optimization(batch_x, batch_y)\n",
        "\n",
        "  if epo % display_epoch == 0:\n",
        "    pred = mlp(batch_x)\n",
        "    loss = cross_entropy_loss(pred, batch_y)\n",
        "    acc = accuracy(pred, batch_y)\n",
        "    print(\"epoch: \", epo, \", loss: \", loss.numpy(), \", acc: \", acc.numpy())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:  2 , loss:  1.5195976 , acc:  0.675\n",
            "epoch:  4 , loss:  0.7965643 , acc:  0.785\n",
            "epoch:  6 , loss:  0.6290409 , acc:  0.815\n",
            "epoch:  8 , loss:  0.5514542 , acc:  0.855\n",
            "epoch:  10 , loss:  0.38754869 , acc:  0.895\n",
            "epoch:  12 , loss:  0.3823639 , acc:  0.89\n",
            "epoch:  14 , loss:  0.42278832 , acc:  0.895\n",
            "epoch:  16 , loss:  0.3665025 , acc:  0.895\n",
            "epoch:  18 , loss:  0.29107288 , acc:  0.9\n",
            "epoch:  20 , loss:  0.24193023 , acc:  0.94\n",
            "epoch:  22 , loss:  0.38297004 , acc:  0.9\n",
            "epoch:  24 , loss:  0.37737808 , acc:  0.885\n",
            "epoch:  26 , loss:  0.31127453 , acc:  0.925\n",
            "epoch:  28 , loss:  0.28469747 , acc:  0.925\n",
            "epoch:  30 , loss:  0.2241109 , acc:  0.925\n",
            "epoch:  32 , loss:  0.24344778 , acc:  0.93\n",
            "epoch:  34 , loss:  0.23047917 , acc:  0.91\n",
            "epoch:  36 , loss:  0.2548714 , acc:  0.92\n",
            "epoch:  38 , loss:  0.24807213 , acc:  0.915\n",
            "epoch:  40 , loss:  0.20051794 , acc:  0.935\n",
            "epoch:  42 , loss:  0.31298012 , acc:  0.92\n",
            "epoch:  44 , loss:  0.35682866 , acc:  0.91\n",
            "epoch:  46 , loss:  0.192667 , acc:  0.95\n",
            "epoch:  48 , loss:  0.21164879 , acc:  0.935\n",
            "epoch:  50 , loss:  0.17177977 , acc:  0.945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTOXI64vjwT0"
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br>\n",
    "<font size=\"6\"><b>Optimization for Machine Learning</b></font>\n",
    "\n",
    "<br><br>\n",
    "<div class=pull-right>\n",
    "By Prof. Seungchul Lee<br>\n",
    "http://iai.postech.ac.kr/<br>\n",
    "Industrial AI Lab at POSTECH\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Optimization\n",
    "\n",
    "- an important tool in 1) engineering problem solving and 2) decision science\n",
    "\n",
    "\n",
    "- optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"./image_files/people_opt.png\" width = 400 ></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__3 key components__\n",
    "1. objective\n",
    "2. decision variable or unknown\n",
    "3. constraints\n",
    "\n",
    "__Procedures__\n",
    "1. The process of identifying objective, variables, and constraints for a given problem is known as \"modeling\"\n",
    "2. Once the model has been formulated, optimization algorithm can be used to find its solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__In mathematical expression__\n",
    "\n",
    "<br>\n",
    "$$\\begin{align*}\n",
    "\\min_{x} \\quad &f(x) \\\\\n",
    "\\text{subject to} \\quad &g_i(x) \\leq 0, \\qquad i=1,\\cdots,m\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "$\\;\\;\\; $where\n",
    "\n",
    "- $x=\\begin{bmatrix}x_1 \\\\ \\vdots \\\\ x_n\\end{bmatrix} \\in \\mathbb{R}^n$ is the decision variable\n",
    "\n",
    "\n",
    "- $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is an objective function\n",
    "\n",
    "\n",
    "- Feasible region: $\\mathcal{C} = \\{x: g_i(x) \\leq 0. \\quad i=1,\n",
    "\\cdots,m\\}$\n",
    "\n",
    "<br><br>\n",
    "Remarks) equivalent\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\min_{x}  f(x) \\quad&\\leftrightarrow \\quad \\max_{x} -f(x)\\\\\n",
    "\\quad g_i(x) \\leq 0\\quad&\\leftrightarrow \\quad -g_i(x) \\geq 0\\\\\n",
    "h(x) = 0 \\quad&\\leftrightarrow \\quad \n",
    "\\begin{cases}\n",
    "  h(x) \\leq 0 \\quad \\text{and} \\\\\n",
    "  h(x) \\geq 0 \n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Solving Optimization Problems\n",
    "\n",
    "- Starting with the unconstrained, one dimensional case\n",
    "\n",
    "<br>\n",
    "<center><img src=\"./image_files/optprob1D.png\" width = 350 ></center><br>\n",
    "\n",
    "\n",
    "- To find minimum point $x^*$, we can look at the derivave of the function $f'(x)$: \n",
    "    - Any location where $f'(x)$ = 0 will be a \"flat\" point in the function\n",
    "   \n",
    "   \n",
    "   \n",
    "- For convex problems, this is guaranteed to be a minimum\n",
    "\n",
    "\n",
    "- Generalization for multivariate function $f:\\mathbb{R}^n \\rightarrow \\ \\mathbb{R}$\n",
    "\n",
    "    - The gradient of $f$ must be zero\n",
    "\n",
    "$$ \\nabla _x f(x) = 0$$\n",
    "\n",
    "\n",
    "- Gradient is a n-dimensional vector containing partial derivatives with respect to each dimension\n",
    "\n",
    "<br>\n",
    "$$\n",
    "x = \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{bmatrix} \\quad \\quad \\quad \\quad \\nabla _x f(x) = \\begin{bmatrix}\n",
    "\\partial f(x) \\over \\partial x_1 \\\\\n",
    "\\vdots\\\\\n",
    "\\partial f(x) \\over \\partial x_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "- For continuously differentiable $f$ and unconstrained optimization, optimal point must have $\\nabla _x f(x^*)=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. How To Find $\\nabla _x f(x) = 0$: Analytic Approach\n",
    "\n",
    "- Direct solution\n",
    "\n",
    "    - In some cases, it is possible to analytically compute $x^*$ such that $ \\nabla _x f(x^*)=0$\n",
    "\n",
    "<br>\n",
    "$$ \n",
    "\\begin{align*}\n",
    "f(x) &= 2x_1^2+ x_2^2 + x_1 x_2 -6 x_1 -5 x_2\\\\\\\\\n",
    "\\Longrightarrow \\nabla _x f(x) &= \\begin{bmatrix}\n",
    "4x_1+x_2-6\\\\\n",
    "2x_2 + x_1 -5\n",
    "\\end{bmatrix} = \\begin{bmatrix}0\\\\0 \\end{bmatrix}\\\\\\\\\n",
    "\\therefore x^* &= \\begin{bmatrix}\n",
    "4 & 1\\\\\n",
    "1 & 2\n",
    "\\end{bmatrix} ^{-1} \\begin{bmatrix}\n",
    "6 \\\\ 5\\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 \\\\ 2\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. How To Find $\\nabla _x f(x) = 0$: Iterative Approach\n",
    "\n",
    "- <font color='red'>Iterative methods</font>\n",
    "\n",
    "   - More commonly the condition that the gradient equal zero will not have an analytical solution, require iterative methods\n",
    "    \n",
    "<br><br>    \n",
    "<center><img src=\"./image_files/iterativemethods.png\" width = 350 ></center>\n",
    "<br><br>\n",
    "\n",
    "- The gradient points in the direction of \"steepest ascent\" for function $f$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Gradient Descent\n",
    "\n",
    "- It motivates the gradient descent algorithm, which repeatedly takes steps in the direction of the negative gradient\n",
    "\n",
    "<br>\n",
    "$$ x \\leftarrow x - \\alpha \\nabla _x f(x) \\quad \\quad \\text{for some step size } \\alpha > 0$$\n",
    "\n",
    "<br><br>\n",
    "<center><img src=\"./image_files/descentdir1d.png\" width = 800 ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient Descent\n",
    "\n",
    "$$\\text{Repeat : } x \\leftarrow x - \\alpha \\nabla _x f(x) \\quad \\quad \\text{for some step size } \\alpha > 0$$ \n",
    "\n",
    "<br><br>\n",
    "<center><img src=\"./image_files/graddesc.png\" width = 400 ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient Descent in Higher Dimension\n",
    "\n",
    "\n",
    "$$\\text{Repeat : } x \\leftarrow x - \\alpha \\nabla _x f(x)$$\n",
    "\n",
    "<br><br><br>\n",
    "<center><img src=\"./image_files/graddescinhigh.png\" width = 900 ></center>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Choosing Step Size \n",
    "\n",
    "- Learning rate\n",
    "\n",
    "<br><br>\n",
    "<center><img src=\"./image_files/ChoosingStep.png\" width = 800 ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Where will We Converge?\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "<center><img src=\"./image_files/wherewillweconv.png\" width = 650 ></center>\n",
    "\n",
    "$\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\qquad \\qquad \\bullet \\, \\text{Random initialization}$\n",
    "\n",
    "$\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\qquad \\qquad \\bullet \\, \\text{Multiple trials}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.2.4. Example\n",
    "\n",
    "<br>\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\min& \\quad (x_1-3)^{2} + (x_2-3)^{2}\\\\\\\\\n",
    "=\\min& \\quad \\frac{1}{2} \\begin{bmatrix} x_1 & x_2 \\end{bmatrix}\n",
    "\\begin{bmatrix} 2 & 0 \\\\ 0 & 2 \\end{bmatrix}\n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} - \n",
    "\\begin{bmatrix} 6 & 6 \\end{bmatrix}\n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} + 18\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\\begin{align*}\n",
    "f &= \\frac{1}{2}X^THX + g^TX \\\\\n",
    "\\nabla f &= HX+g\n",
    "\\end{align*}\n",
    "\n",
    "- update rule\n",
    "\n",
    "$$ X_{i+1} = X_{i} - \\alpha_i \\nabla f(X_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T21:57:29.506092Z",
     "start_time": "2021-07-26T21:57:29.366605Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.99999147]\n",
      " [2.99999147]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "H = np.matrix([[2,0], [0,2]])\n",
    "g = np.matrix([[6], [6]])\n",
    "\n",
    "x = np.zeros((2,1))\n",
    "alpha = 0.1\n",
    "\n",
    "for i in range(25):    \n",
    "    df = H*x + g\n",
    "    x = x - alpha * df\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5. Example\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\min& \\quad x_1^2 + x_2^2 + x_3^2 - 6x_1 + 2x_3 + 10\\\\\\\\\n",
    "=\\min& \\quad \\frac{1}{2} \\begin{bmatrix} x_1 & x_2 & x_3\\end{bmatrix}\n",
    "\\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 2\\end{bmatrix}\n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} + \n",
    "\\begin{bmatrix} -6 & 0 & 2 \\end{bmatrix}\n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} + 10\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\\begin{align*}\n",
    "f &= \\frac{1}{2}X^THX + g^TX \\\\\n",
    "\\nabla f &= HX+g\n",
    "\\end{align*}\n",
    "\n",
    "- update rule\n",
    "\n",
    "$$ X_{i+1} = X_{i} - \\alpha_i \\nabla f(X_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T21:57:29.522075Z",
     "start_time": "2021-07-26T21:57:29.507088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.99999147]\n",
      " [ 0.        ]\n",
      " [-0.99999716]]\n"
     ]
    }
   ],
   "source": [
    "H = np.array([[2,0,0], 22])\n",
    "f = \n",
    "\n",
    "x =  \n",
    "alpha = \n",
    "\n",
    "for i in range(25):    \n",
    "    g = \n",
    "    x = \n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.6. Example\n",
    "\n",
    "<br>\n",
    "$$\n",
    "\\begin{array}{Icr}\\begin{align*}\n",
    "\\max_{x} \\quad & 3x_1 + {3 \\over 2}x_2 \\\\\n",
    "\\text{subject to} \\quad\n",
    "& -1 \\leq x_1 \\leq 2 \\\\\n",
    "& \\quad 0 \\leq x_2 \\leq 3\n",
    "\\end{align*}\\end{array}\n",
    "\\quad\\implies\\quad\n",
    "\\begin{array}{I}\n",
    "\\quad \\min_{x} \\quad & - \\begin{bmatrix} 3 \\\\ 3 / 2 \\end{bmatrix}^T \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\\\\n",
    "\\text{subject to} \\quad\n",
    "& \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix} \\leq \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\leq \\begin{bmatrix} 2 \\\\ 3 \\\\ \\end{bmatrix}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T21:57:29.538005Z",
     "start_time": "2021-07-26T21:57:29.523046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.]\n",
      " [3.]]\n"
     ]
    }
   ],
   "source": [
    "f = \n",
    "\n",
    "x = \n",
    "alpha = \n",
    "\n",
    "lb = \n",
    "ub = \n",
    "\n",
    "for i in range(25): \n",
    "    x = \n",
    "    \n",
    "    # lb constraints \n",
    "    lb_TF = \n",
    "    x = \n",
    "    \n",
    "    # ub constraints \n",
    "    ub_TF = \n",
    "    x = \n",
    "    \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T21:57:29.553962Z",
     "start_time": "2021-07-26T21:57:29.539003Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%ewwqerqwerwe%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "217px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
